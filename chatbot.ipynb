{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3201f2d",
   "metadata": {},
   "source": [
    "# Enterprise ChatBot using RAG with LangChain and OpenAI \n",
    "\n",
    "This repository contains a sample implementation of an enterprise chatbot using Retrieval-Augmented Generation (RAG) with LangChain and OpenAI. The chatbot is designed to answer questions based on a set of documents, providing accurate and contextually relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea15a4",
   "metadata": {},
   "source": [
    "## Features\n",
    "- Document Ingestion: Load and process documents from various sources.\n",
    "- Vector Store: Use FAISS to store and retrieve document embeddings.\n",
    "- Language Model: Utilize OpenAI's GPT-5-nano for generating responses.\n",
    "- Conversational Memory: Maintain context across multiple interactions.\n",
    "- User Interface: Simple command-line interface for interaction.\n",
    "## Requirements\n",
    "- Python 3.8+\n",
    "- OpenAI API Key\n",
    "- Required Python packages (see `requirements.txt`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3866a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.agents import AgentType, initialize_agent, Tool, AgentExecutor, create_openai_tools_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import document, SystemMessage\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.prompts import PromptTemplate,  ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain import hub\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176cbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP SOME CONSTANTS\n",
    "MODEL = 'gpt-5-nano'\n",
    "db_name = 'company_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c22217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load enviroment variables (open ai api key)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954850c",
   "metadata": {},
   "source": [
    "<!-- ### Read the Docs and split them into chunks -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58369c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\company', 'data\\\\employees', 'data\\\\products']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 86.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\company\\\\ColumbusAI_Solutions.md'}, page_content='# Company Record\\n\\n# ColumbusAI Solutions\\n\\n## Overview\\nColumbusAI Solutions is a global technology firm focused on artificial intelligence, automation, and data-driven consulting. Our mission is to empower enterprises to achieve autonomous operations and data-driven decisions through trustworthy AI. Our vision is to become a leading partner for organizations pursuing scalable, ethical, and responsible AI at every layer of the enterprise. Core focus: AI-driven automation, software and data deployment, predictive analytics, AI consulting and strategy, and custom model development. We develop and deploy AI-based products for automation, business intelligence, and enterprise digital transformation.\\n\\n## History and Foundation\\nColumbusAI Solutions was founded in 2016 by Dr. Elena Koslov, a former AI scientist with deep experience in enterprise-scale data platforms. The founding team launched the company in Chicago with the aim of bridging advanced research and practical adoption for mid-market and Fortune 500 customers. Early engagements focused on automation pilots for manufacturing and logistics. By 2018 the company expanded to a blended services-and-products model, and by 2020 it secured seed funding to accelerate product development. In 2021 a New York City headquarters was established and a dedicated global delivery network was formed. From a local AI startup, ColumbusAI Solutions grew into an international consulting and product company with regional hubs and a nearshoring capability across the Americas.\\n\\n## Core Business Areas\\n- AI-driven automation\\n- Software and data deployment\\n- Predictive analytics and business intelligence\\n- AI consulting and strategy\\n- Custom model development\\n\\n## Product Ecosystem\\nColumbusAI Solutions operates a cohesive product portfolio designed to be deployed in stages across client environments. The suite includes 10 products that integrate via a common orchestration layer and data fabric:\\n\\n1) AtlasFlow - AI-driven automation platform for workflow orchestration and RPA\\n2) PulseBI - enterprise business intelligence and analytics workspace\\n3) NexusPredict - predictive analytics for demand, maintenance, and risk\\n4) SentinelMLOps - model lifecycle management and governance\\n5) DataForge - data orchestration and integration fabric\\n6) KernelEdge - edge AI for manufacturing and field devices\\n7) InsightForge - analytics storytelling and reporting layer\\n8) StreamlineRPA - robotic process automation tooling\\n9) CogniaDS - data science services and collaborative notebooks\\n10) AuroraGuard - AI governance, security, and compliance\\n\\nOur internal technologies include the Columbus Orchestrator (model orchestration system), AstraPipe (end-to-end data pipelines), and Nova Studio (low-code tooling) to assemble solutions rapidly. We design solutions that span data ingestion, model deployment, monitoring, and business logic integration.\\n\\n## Global Presence\\nHeadquartered in New York City, with regional hubs in London and Singapore, and R&D and regional delivery centers in Sao Paulo and Mexico City. The company maintains client delivery capabilities across North America, Europe, and Latin America, with partnerships to support clients in additional markets.\\n\\n## Key Achievements\\n- Strategic partnerships with major cloud platforms and technology providers (Azure, AWS, Google Cloud) enabling hybrid deployments and accelerated time-to-value.\\n- Certifications including ISO 27001 and SOC 2 Type II, plus AI ethics and governance recognitions.\\n- Delivered enterprise-grade digital transformation projects for Fortune 500 clients, achieving multi-year ROI and significant operational efficiency gains.\\n- Recognitions for responsible AI practices and innovation in automation.\\n\\n## Company Culture and Values\\nColumbusAI Solutions champions innovation, transparency, inclusion, and sustainability. The company maintains an active ethics and governance board for AI projects, runs transparent career progression and mentoring programs, and supports sustainable operations across offices (green energy, carbon reporting, and local community initiatives).\\n\\n## Financial or Growth Highlights\\n- Revenue grew at an average of 26% CAGR over the last three fiscal years.\\n- Serves 120+ enterprise clients across industries including manufacturing, financial services, retail, and healthcare.\\n- Year-over-year expansion in service delivery capacity averaging 18-22% annually, with ongoing investments in product R&D and global delivery.\\n\\n## Future Vision\\nIn the next 3-5 years, ColumbusAI Solutions will scale generative AI and automation across the enterprise, building deeper governance and security controls, expanding to additional regions (Africa, Middle East), and growing our cloud-agnostic product stack. We will advance platform-level capabilities for AI-assisted decision making, scalable automation at the edge and in the cloud, and a broader partner ecosystem to accelerate digital transformation across industries.\\n\\nColumbusAI Solutions is shaping the future of intelligent automation and enterprise AI transformation by turning advanced AI research into practical, responsible, and scalable solutions that empower organizations to operate more intelligently and efficiently.\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 94.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\employees\\\\Ernesto Jimenez.md'}, page_content='# HR Record\\n\\n# Ernesto Jimenez\\n\\n## Summary\\n- **Date of Birth:** 1989-03-22\\n- **Job Title:** Software Engineer\\n- **Location:** Madrid, Spain\\n\\n## Professional Trajectory\\n- **2014** - Software Engineer at CodePulse; backend services and APIs.\\n- **2018** - Senior Software Engineer at TechWave; led microservices migration and performance optimization.\\n- **2021** - Backend Engineer at AI Solutions Ltd; designed scalable services and API contracts.\\n- **Current** - Senior Software Engineer at our company; focuses on distributed systems, containerized deployments, and cloud-native architecture.\\n\\n## Education & Certifications\\n- BSc in Computer Science, University of Valencia\\n- AWS Solutions Architect – Associate, 2019\\n- Google Cloud Professional Cloud Developer, 2020\\n\\n## Awards & Recognitions\\n- Hackathon Winner, Global Hack Challenge 2020\\n- Open Source Contributor of the Year 2022\\n\\n## Skills\\n- Languages: Python, Go, Java\\n- Frameworks: Spring Boot, FastAPI\\n-Cloud: AWS, GCP; Kubernetes; Docker\\n- Microservices, RESTful APIs, distributed systems\\n'), Document(metadata={'source': 'data\\\\employees\\\\Helena Suarez.md'}, page_content='# HR Record\\n\\n# Helena Suarez\\n\\n## Summary\\n- **Date of Birth:** 1988-11-02\\n- **Job Title:** Data Engineer\\n- **Location:** Barcelona, Spain\\n\\n## Professional Trajectory\\n- **2012** - Data Analyst at FinTech Co; built data pipelines and dashboards.\\n- **2015** - Senior Data Engineer at CloudForge Ltd; established streaming ETL with Kafka, Spark, and Airflow.\\n- **2019** - Data Engineer at analytics firm; led migration to cloud data lake on AWS.\\n- **Current** - Senior Data Engineer at our company; responsible for scalable data platforms, data ingestion, governance, and performance optimization.\\n\\n## Education & Certifications\\n- BSc in Computer Science, University of Barcelona\\n- MSc in Data Engineering, Open University (UK)\\n- Google Cloud Professional Data Engineer, 2020\\n- AWS Certified Data Analytics – Specialty, 2021\\n\\n## Awards & Recognitions\\n- 2022 Data Engineer of the Year by DataTech Awards\\n\\n## Skills\\n- Python, SQL, Spark, Hadoop\\n- Cloud platforms: AWS, GCP\\n- Data modeling, ETL/ELT, streaming pipelines\\n- Data governance and security\\n'), Document(metadata={'source': 'data\\\\employees\\\\Juan Rojas.md'}, page_content='# HR Record\\n\\n# Juan Rojas\\n\\n## Summary\\n- **Date of Birth:** April 12, 1979\\n- **Job Title:** CEO\\n- **Location:** San Francisco, CA\\n\\n## Leadership & Strategy\\n- **2012** - Joined TechNova AI as Product Manager, led cross-functional teams in launching two ML-powered products.\\n- **2015** - Promoted to Chief Operating Officer at TechNova AI; oversaw operations, finance, and partnerships.\\n- **2020** - Joined our AI company as Chief Executive Officer; scaled the organization from 25 to 180+ employees and led major fundraising rounds.\\n- **Current** - Driving strategic vision, corporate governance, and international expansion.\\n\\n## Education & Certifications\\n- **MBA**, Stanford Graduate School of Business (2012)\\n- **B.Sc. in Computer Science**, University of Techland (2003)\\n- **Executive Education**, AI Product Leadership, Wharton Online (2020)\\n\\n## Awards & Recognitions\\n- 2021 Entrepreneur of the Year by AI Forum\\n- 2023 Technology Leadership Award by Global Tech Council\\n\\n## Highlights\\n- Led fundraising rounds totaling over 75M in capital.\\n- Built and scaled cross-border operations across three regions.\\n'), Document(metadata={'source': 'data\\\\employees\\\\Samuel Etoo.md'}, page_content='# HR Record\\n\\n# Samuel Etoo\\n\\n## Summary\\n- **Date of Birth:** 1984-05-11\\n- **Job Title:** Accountant\\n- **Location:** Douala, Cameroon\\n\\n## Professional Trajectory\\n- **2010** - Accounts Payable Clerk at FinServe Ltd; processed invoices and payments.\\n- **2015** - Financial Analyst at GlobalCorp; prepared budgets and variance analysis.\\n- **2018** - Senior Accountant at GlobalCorp; managed month-end close and compliance.\\n- **2021** - Controller at TechFinance; led audits and financial planning.\\n- **Current** - Senior Accountant at our company; oversees GAAP compliance, consolidation, and reporting.\\n\\n## Education & Certifications\\n- BBA in Accounting, University of Douala\\n- CPA, 2012\\n- IFRS Certification, 2016\\n\\n## Awards & Recognitions\\n- 2020 Finance Team of the Year at GlobalCorp\\n\\n## Skills\\n- GAAP, IFRS, FP&A, SAP, Oracle\\n- Financial reporting, budgeting, variance analysis\\n- Tax compliance and audit readiness\\n'), Document(metadata={'source': 'data\\\\employees\\\\Santiago Rojas.md'}, page_content='# HR Record\\n\\n# Santiago Rojas\\n\\n## Summary\\n- **Date of Birth:** 1990-07-14\\n- **Job Title:** Data Analyst\\n- **Location:** Medellín, Colombia\\n\\n## Professional Trajectory\\n- **2016** - Junior Data Analyst at MarketPulse; supported data cleaning and reporting.\\n- **2019** - Data Analyst at GrowthAnalytics; built dashboards and automated reporting using Tableau.\\n- **2021** - Senior Data Analyst at our company; improved forecasting accuracy by streamlining data pipelines and introducing data storytelling.\\n- **Current** - Data Analyst focusing on insights, experimentation, and cross-team collaboration.\\n\\n## Education & Certifications\\n- BSc in Statistics, University of Medellín\\n- Tableau Desktop Specialist, 2020\\n- Google Data Analytics Professional Certificate, 2021\\n\\n## Awards & Recognitions\\n- 2020 Best Analytics Project at GrowthAnalytics\\n\\n## Skills\\n- SQL, Python, R, Tableau, Power BI\\n- Data visualization, statistical analysis, forecasting\\n- Data wrangling, ETL, data storytelling\\n')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 94.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\products\\\\AtlasFlow.md'}, page_content='# Product Record\\n\\n# AtlasFlow\\n\\n## Overview\\nAtlasFlow is an AI-driven automation platform for workflow orchestration and robotic process automation (RPA). It enables enterprises to design, deploy, and monitor automated processes across cloud, on-prem, and hybrid environments. It targets IT operations, business process owners, and developers who want reliable, auditable automation at scale.\\n\\n## Core Features\\n- AI-powered orchestration engine with deterministic scheduling and human-in-the-loop capabilities\\n- RPA bot lifecycle management with versioning, fault tolerance, and centralized governance\\n- AI-assisted action inference using OpenAI API and LangChain for natural-language automation and intent recognition\\n- Cloud-native deployment with Kubernetes, Git-based pipelines, and native connectors to Salesforce, SAP, and Power BI integration\\n- Policy-based governance, RBAC, auditing, and compliance reporting\\n\\n## Use Cases\\n- Finance and accounting: AP/AR automation, invoice processing, and reconciliation\\n- IT operations: incident response automation and patch management\\n- Manufacturing: shop-floor data collection and device orchestration across MES and ERP systems\\n\\n## Development Timeline\\n- v1.0 (Q3 2024): core workflow designer, RPA runtime, and connectors\\n- v1.5 (Q2 2025): AI-assisted action planning, governance module, and expanded connectors\\n- v2.0 (Q4 2026): full observability suite, policy automation, and cross-cloud orchestration\\n\\n## Performance or Success Metrics\\n- Reduced task completion time by 40–60% across automated processes\\n- Increased automation coverage by 50–70% within the first year\\n- Decrease in manual errors for critical processes by up to 80%\\n\\n## Pricing and Availability\\n- Subscription-based with enterprise license; on-premise deployment available for regulated industries\\n\\n---\\n\\n**Comment:** AtlasFlow exemplifies our strategy of enabling scalable, AI-powered automation with strong governance to accelerate digital transformation across industries.'), Document(metadata={'source': 'data\\\\products\\\\AuroraGuard.md'}, page_content='# Product Record\\n\\n# AuroraGuard\\n\\n## Overview\\nAuroraGuard delivers AI governance, security, and compliance across the data-to-model lifecycle. It provides policy-driven governance, risk scoring, privacy controls, and audit-ready reporting to help organizations operate responsible AI at scale.\\n\\n## Core Features\\n- Policy engine for model governance, data usage, and access control\\n- Model risk scoring, bias detection, and impact assessment tooling\\n- Privacy-preserving features, data minimization, and differential privacy options\\n- Audit trails, activity logging, and reproducible governance reports\\n- Integration with existing security and identity platforms (IAM, SIEM) for centralized control\\n\\n## Use Cases\\n- Regulated industries requiring strict governance and auditability\\n- Enterprise AI programs needing risk and compliance visibility\\n- Data managers seeking privacy controls and data lineage\\n\\n## Development Timeline\\n- v1.0 (Q2 2024): policy framework and basic governance dashboards\\n- v1.3 (Q3 2025): bias detection, privacy controls, and audit reporting\\n- v2.0 (Q4 2026): enterprise-grade cross-cloud governance and automated risk remediation\\n\\n## Performance or Success Metrics\\n- Policy coverage reached > 95% of critical AI workflows\\n- Audit readiness achieved > 99% pass rate in internal checks\\n- Privacy controls effective in preventing data leakage incidents\\n\\n## Pricing and Availability\\n- Enterprise license with security/compliance add-ons\\n\\n---\\n\\n**Comment:** AuroraGuard aligns with our strategy to build trustworthy AI foundations, ensuring governance, security, and compliance are central to every product and deployment.'), Document(metadata={'source': 'data\\\\products\\\\CogniaDS.md'}, page_content='# Product Record\\n\\n# CogniaDS\\n\\n## Overview\\nCogniaDS delivers data science services and collaborative notebooks to accelerate AI-ready data science workstreams. It combines managed notebooks, collaborative workflows, and expert consulting to help data teams prototype, validate, and operationalize models.\\n\\n## Core Features\\n- Managed notebooks (Jupyter-like) with collaboration, sharing, and access controls\\n- AutoML workflows, experiment tracking, and model evaluation dashboards\\n- Consulting services for problem framing, data preparation, and feature engineering\\n- Reproducible pipelines with containerized environments and versioned assets\\n- Notebooks integration for team-wide collaboration and knowledge sharing\\n\\n## Use Cases\\n- R&D and product teams prototyping ML models\\n- Data science enablement programs with internal knowledge sharing\\n- MLOps handoffs from research to production\\n\\n## Development Timeline\\n- v1.0 (Q2 2024): collaborative notebooks and basic consulting templates\\n- v1.2 (Q1 2025): automated experiment tracking and model evaluation dashboards\\n- v2.0 (Q3 2026): enterprise governance, asset catalog, and broader collaboration features\\n\\n## Performance or Success Metrics\\n- Time-to-first-model reduced by 40–50%\\n- Collaboration adoption rate above 70%\\n- Model reproducibility score > 95%\\n\\n## Pricing and Availability\\n- Enterprise license with optional consulting engagements\\n\\n---\\n\\n**Comment:** CogniaDS embodies our people-first approach, enabling data teams to move faster together with reusable, expert-guided workflows.'), Document(metadata={'source': 'data\\\\products\\\\DataForge.md'}, page_content='# Product Record\\n\\n# DataForge\\n\\n## Overview\\nDataForge is a data orchestration and integration fabric that bridges cloud and on-prem data sources, enabling reliable data pipelines, quality checks, and end-to-end observability. It serves data engineers, analysts, and data stewards who need clean, timely data across the organization.\\n\\n## Core Features\\n- Dataflow orchestration with dependency graphs and retry policies (Dagster/Dagster-like experience)\\n- Wide range of connectors to SaaS apps, databases, data lakes, and messaging systems\\n- Data quality checks, schema evolution handling, and lineage visualization\\n- Real-time streaming and batch processing with scalable orchestration\\n- Observability dashboards for data freshness, SLA tracking, and error remediation\\n\\n## Use Cases\\n- Data lake modernization and ETL modernization\\n- Cross-domain data integration for analytics and ML\\n- Real-time analytics with streaming data sources\\n\\n## Development Timeline\\n- v1.0 (Q2 2024): core orchestration engine and 20+ connectors\\n- v1.3 (Q3 2025): data quality and lineage features\\n- v2.0 (Q4 2026): enterprise-grade governance, multi-region deployment\\n\\n## Performance or Success Metrics\\n- Data integration lead time reduced by 50%\\n- Data quality pass rate improved to 99.9%\\n- SLA compliance for data freshness achieved in 95% of pipelines\\n\\n## Pricing and Availability\\n- Subscription with optional on-premise deployment\\n\\n---\\n\\n**Comment:** DataForge anchors our data-centric strategy by providing reliable, observable data pipelines that power analytics and AI workloads across the enterprise.'), Document(metadata={'source': 'data\\\\products\\\\InsightForge.md'}, page_content='# Product Record\\n\\n# InsightForge\\n\\n## Overview\\nInsightForge is an analytics storytelling and reporting layer that converts data findings into compelling, narrative-rich insights. It focuses on automated narrative generation, data-to-text, and visually engaging storytelling to empower non-technical stakeholders.\\n\\n## Core Features\\n- Narrative-driven analytics with automatic data-to-text generation\\n- Rich storyboarding, dashboards, and auto-generated executive briefings\\n- Natural language generation for reports, emails, and slide decks\\n- BI-native integrations and scripting support for custom analytics pipelines\\n- Collaboration, versioning, and approval workflows for reports\\n\\n## Use Cases\\n- Executive-level monthly/quarterly business reviews\\n- Investor relations, marketing storytelling, and customer insights\\n- Product analytics summaries for non-technical audiences\\n\\n## Development Timeline\\n- v1.0 (Q1 2024): storytelling templates and narrative generation\\n- v1.2 (Q4 2024): multi-source storytelling and collaboration features\\n- v2.0 (Q2 2026): AI-assisted insights recommendations and governance\\n\\n## Performance or Success Metrics\\n- Time-to-story generation reduced by 70%\\n- Story accuracy and relevance ratings exceed 95%\\n- User satisfaction with narrative outputs > 90%\\n\\n## Pricing and Availability\\n- Subscription-based with enterprise license\\n\\n---\\n\\n**Comment:** InsightForge complements our analytics stack by turning data into readable, persuasive narratives that drive action across the organization.'), Document(metadata={'source': 'data\\\\products\\\\KernelEdge.md'}, page_content='# Product Record\\n\\n# KernelEdge\\n\\n## Overview\\nKernelEdge brings AI to the edge with optimized on-device inference for manufacturing and field devices. It enables low-latency decisions, offline operation, and secure model updates, reducing reliance on centralized cloud processing for mission-critical environments.\\n\\n## Core Features\\n- Edge AI inference with model optimization (quantization, pruning) for low-latency runtimes\\n- Local decision-making and autonomous control for factory devices and remote sensors\\n- Over-the-air (OTA) model updates, secure boot, and tamper-evident logging\\n- Edge security hardening and hardware-accelerated inference\\n- Seamless cloud-to-edge deployment with centralized model management\\n\\n## Use Cases\\n- Predictive maintenance and anomaly detection on shop floors\\n- Real-time quality control and process optimization for manufacturing lines\\n- Remote monitoring in field services and energy installations\\n\\n## Development Timeline\\n- v1.0 (Q3 2024): edge runtimes and OTA updates\\n- v1.5 (Q1 2025): expanded hardware support and security features\\n- v2.0 (Q3 2026): multi-edge orchestration and federated learning capabilities\\n\\n## Performance or Success Metrics\\n- Latency for on-device inference under 50 ms\\n- 99.9% uptime for edge devices in remote environments\\n- Bandwidth efficiency improvements of 70% by processing locally\\n\\n## Pricing and Availability\\n- Device-licensing and subscription-based model; enterprise deployments available\\n\\n---\\n\\n**Comment:** KernelEdge extends our AI capabilities to the physical world, enabling resilient, latency-sensitive intelligence at the edge in line with our strategy of ubiquitous AI.'), Document(metadata={'source': 'data\\\\products\\\\NexusPredict.md'}, page_content='# Product Record\\n\\n# NexusPredict\\n\\n## Overview\\nNexusPredict delivers predictive analytics for demand forecasting, preventive maintenance, and risk assessment. Built for operations teams, supply chain planners, and risk managers, it combines time-series modeling, anomaly detection, and scenario planning to optimize decisions under uncertainty.\\n\\n## Core Features\\n- Time-series forecasting with ensemble models (Prophet, TensorFlow/Keras, and scikit-learn-based methods)\\n- Anomaly detection and drift monitoring across data streams\\n- Predictive maintenance with asset-level health scores and maintenance windows\\n- Scenario planning and what-if simulations with fast iteration\\n- Data connectors to ERP, MES, and data lakes; alerting and integration with alerting/alerting platforms\\n\\n## Use Cases\\n- Retail and manufacturing demand planning\\n- Fleet and equipment maintenance optimization in industrial settings\\n- Risk assessment for finance and supply chain disruptions\\n\\n## Development Timeline\\n- v0.9 (Q2 2024): core forecasting engine and data connectors\\n- v1.0 (Q4 2024): standard dashboards and anomaly detection\\n- v1.5 (Q3 2025): scenario planning and automated alerting\\n- v2.0 (Q4 2026): enterprise-grade governance and deployment options\\n\\n## Performance or Success Metrics\\n- Forecast accuracy improved by 15–25% across key SKUs and assets\\n- Reduction in unplanned downtime by 20–30%\\n- Lead-time for demand planning shortened by 40%\\n\\n## Pricing and Availability\\n- Subscription-based with enterprise license; on-premise deployment available for sensitive data environments\\n\\n---\\n\\n**Comment:** NexusPredict aligns with our mission to turn data into proactive, risk-aware decisions across the value chain.'), Document(metadata={'source': 'data\\\\products\\\\PulseBI.md'}, page_content='# Product Record\\n\\n# PulseBI\\n\\n## Overview\\nPulseBI is an enterprise business intelligence and analytics workspace that unifies data sources, supports self-service analytics, and accelerates decision-making with AI-powered insights. It serves executives, data analysts, and line-of-business users who need trusted data and faster storytelling capabilities.\\n\\n## Core Features\\n- Central analytics workspace with semantic layer and data catalog for self-service dashboards\\n- AI-assisted insights and natural language querying using OpenAI API and LangChain\\n- Seamless Power BI integration and cloud-based deployment with secure data sharing\\n- Data lineage, governance, and role-based access control across connected data sources\\n- Collaborative storytelling with annotation and versioned dashboards\\n\\n## Use Cases\\n- Executive and board reporting for sales, finance, and operations\\n- Marketing analytics and campaign optimization\\n- Supply chain visibility and KPI tracking across multi-cloud data sources\\n\\n## Development Timeline\\n- v1.0 (Q4 2024): core analytics workspace, data connectors, and semantic layer\\n- v1.2 (Q3 2025): AI-driven insights, NLQ, and collaboration features\\n- v2.0 (Q1 2026): advanced data catalog, governance enhancements, and deeper Power BI integration\\n\\n## Performance or Success Metrics\\n- Time-to-insight reduced by 60% for typical business questions\\n- Dashboard refresh latency under 2 minutes for primary sources\\n- User adoption growth of 40% quarter-over-quarter\\n\\n## Pricing and Availability\\n- Subscription-based with enterprise license; on-premise options for regulated environments\\n\\n---\\n\\n**Comment:** PulseBI reinforces our vision of empowering all users with trusted, AI-enhanced analytics and collaborative storytelling at scale.'), Document(metadata={'source': 'data\\\\products\\\\SentinelMLOps.md'}, page_content='# Product Record\\n\\n# SentinelMLOps\\n\\n## Overview\\nSentinelMLOps provides comprehensive model lifecycle management and governance for AI initiatives. It covers versioning, lineage, compliance, reproducibility, and automated deployment pipelines to ensure safe, auditable AI across organizations.\\n\\n## Core Features\\n- Model registry, versioning, lineage, and provenance tracking\\n- CI/CD for ML models with automated testing, validation, and canary deployments\\n- Governance suite including model risk scoring, bias detection, and privacy controls\\n- Reproducibility tooling with experiment tracking and containerized environments (Docker/Kubernetes)\\n- Security scanning, credential management, and deployment hardening\\n\\n## Use Cases\\n- Regulated industries (finance, healthcare) requiring auditable AI\\n- Enterprise AI initiatives with multi-model governance and compliance needs\\n- MLOps teams aiming to accelerate safe deployment and rollback\\n\\n## Development Timeline\\n- v1.0 (Q1 2024): model registry, basic deployment pipelines, and reproducibility\\n- v1.3 (Q2 2025): governance and bias auditing capabilities\\n- v2.0 (Q3 2026): full enterprise-scale deployment, security, and cross-cloud support\\n\\n## Performance or Success Metrics\\n- Time-to-prod for models reduced by 40%\\n- Compliance audits achieved with 95% pass rate\\n- Rollback incidents and failed deployments reduced by 70%\\n\\n## Pricing and Availability\\n- Enterprise license with optional on-premise deployment\\n\\n---\\n\\n**Comment:** SentinelMLOps embodies our commitment to responsible AI, providing the governance backbone that lets customers trust and scale their models.'), Document(metadata={'source': 'data\\\\products\\\\StreamlineRPA.md'}, page_content='# Product Record\\n\\n# StreamlineRPA\\n\\n## Overview\\nStreamlineRPA provides robust robotic process automation tooling focused on scalable bot design, process discovery, and unattended/autonomous automation. It enables rapid automation of repetitive tasks with strong orchestration and monitoring capabilities.\\n\\n## Core Features\\n- Visually rich RPA designer with process discovery and workload balancing\\n- Attended and unattended bot execution with centralized monitoring\\n- AI-assisted automation suggestions and intent detection for rapid bot creation\\n- Cross-application automation with connectors to ERP, CRM, and collaboration tools\\n- Logging, auditing, and performance dashboards for governance\\n\\n## Use Cases\\n- HR onboarding and employee data processing\\n- Accounts payable and invoice processing\\n- Customer support ticket routing and triage\\n\\n## Development Timeline\\n- v1.0 (Q4 2024): bot designer, runtime, and connectors\\n- v1.3 (Q3 2025): process discovery, monitoring, and analytics\\n- v2.0 (Q4 2026): autonomous bot orchestration and self-healing capabilities\\n\\n## Performance or Success Metrics\\n- Automation coverage up to 60–70% of target processes\\n- Reduction in manual handling time by 30–50%\\n- Bot failure rate below 1%\\n\\n## Pricing and Availability\\n- Subscription-based with enterprise licensing\\n\\n---\\n\\n**Comment:** StreamlineRPA strengthens our automation portfolio by enabling scalable, practical automation across business units with measurable impact.')]\n",
      "total number of chunks 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read documents using LangChain loaders\n",
    "# Take everythin in all the sub-folder of the data\n",
    "\n",
    "folders = [f for f in glob.glob(r\"data/*\") if not f.endswith('.ipynb')]\n",
    "\n",
    "print(folders)\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata['doc_type'] = doc_type\n",
    "    return doc\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob= \"**/*.md\", loader_cls= TextLoader, loader_kwargs= text_loader_kwargs, show_progress= True)\n",
    "    folder_docs = loader.load()\n",
    "    print(folder_docs)\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500, \n",
    "                                      chunk_overlap = 200,\n",
    "                                      separators= [\"\\n\\n\",\"\\n\",\".\" ,\" \", \"\"])\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'total number of chunks {len(chunks)}')\n",
    "# print(f'Document types found: {set()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddcd96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 29 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chuncks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory= db_name, embedding_function= embeddings).delete_collection()\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding= embeddings, persist_directory= db_name)\n",
    "collection = vectorstore._collection\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc138510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework (with thanks to Jon R for identifying and fixing a bug in this!)\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10416b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": [
           "orange",
           "orange",
           "orange",
           "orange",
           "orange",
           "green",
           "green",
           "green",
           "green",
           "green",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue",
           "blue"
          ],
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers",
         "text": [
          "Type: company<br>Text: # Company Record\n\n# ColumbusAI Solutions\n\n## Overview\nColumbusAI Solutions is a global technology fi...",
          "Type: company<br>Text: ## History and Foundation\nColumbusAI Solutions was founded in 2016 by Dr. Elena Koslov, a former AI ...",
          "Type: company<br>Text: 1) AtlasFlow - AI-driven automation platform for workflow orchestration and RPA\n2) PulseBI - enterpr...",
          "Type: company<br>Text: ## Key Achievements\n- Strategic partnerships with major cloud platforms and technology providers (Az...",
          "Type: company<br>Text: ## Future Vision\nIn the next 3-5 years, ColumbusAI Solutions will scale generative AI and automation...",
          "Type: employees<br>Text: # HR Record\n\n# Ernesto Jimenez\n\n## Summary\n- **Date of Birth:** 1989-03-22\n- **Job Title:** Software...",
          "Type: employees<br>Text: # HR Record\n\n# Helena Suarez\n\n## Summary\n- **Date of Birth:** 1988-11-02\n- **Job Title:** Data Engin...",
          "Type: employees<br>Text: # HR Record\n\n# Juan Rojas\n\n## Summary\n- **Date of Birth:** April 12, 1979\n- **Job Title:** CEO\n- **L...",
          "Type: employees<br>Text: # HR Record\n\n# Samuel Etoo\n\n## Summary\n- **Date of Birth:** 1984-05-11\n- **Job Title:** Accountant\n-...",
          "Type: employees<br>Text: # HR Record\n\n# Santiago Rojas\n\n## Summary\n- **Date of Birth:** 1990-07-14\n- **Job Title:** Data Anal...",
          "Type: products<br>Text: # Product Record\n\n# AtlasFlow\n\n## Overview\nAtlasFlow is an AI-driven automation platform for workflo...",
          "Type: products<br>Text: ## Performance or Success Metrics\n- Reduced task completion time by 40–60% across automated processe...",
          "Type: products<br>Text: # Product Record\n\n# AuroraGuard\n\n## Overview\nAuroraGuard delivers AI governance, security, and compl...",
          "Type: products<br>Text: ## Pricing and Availability\n- Enterprise license with security/compliance add-ons\n\n---\n\n**Comment:**...",
          "Type: products<br>Text: # Product Record\n\n# CogniaDS\n\n## Overview\nCogniaDS delivers data science services and collaborative ...",
          "Type: products<br>Text: ## Pricing and Availability\n- Enterprise license with optional consulting engagements\n\n---\n\n**Commen...",
          "Type: products<br>Text: # Product Record\n\n# DataForge\n\n## Overview\nDataForge is a data orchestration and integration fabric ...",
          "Type: products<br>Text: ## Pricing and Availability\n- Subscription with optional on-premise deployment\n\n---\n\n**Comment:** Da...",
          "Type: products<br>Text: # Product Record\n\n# InsightForge\n\n## Overview\nInsightForge is an analytics storytelling and reportin...",
          "Type: products<br>Text: ## Pricing and Availability\n- Subscription-based with enterprise license\n\n---\n\n**Comment:** InsightF...",
          "Type: products<br>Text: # Product Record\n\n# KernelEdge\n\n## Overview\nKernelEdge brings AI to the edge with optimized on-devic...",
          "Type: products<br>Text: ## Pricing and Availability\n- Device-licensing and subscription-based model; enterprise deployments ...",
          "Type: products<br>Text: # Product Record\n\n# NexusPredict\n\n## Overview\nNexusPredict delivers predictive analytics for demand ...",
          "Type: products<br>Text: ## Performance or Success Metrics\n- Forecast accuracy improved by 15–25% across key SKUs and assets\n...",
          "Type: products<br>Text: # Product Record\n\n# PulseBI\n\n## Overview\nPulseBI is an enterprise business intelligence and analytic...",
          "Type: products<br>Text: ## Pricing and Availability\n- Subscription-based with enterprise license; on-premise options for reg...",
          "Type: products<br>Text: # Product Record\n\n# SentinelMLOps\n\n## Overview\nSentinelMLOps provides comprehensive model lifecycle ...",
          "Type: products<br>Text: ## Pricing and Availability\n- Enterprise license with optional on-premise deployment\n\n---\n\n**Comment...",
          "Type: products<br>Text: # Product Record\n\n# StreamlineRPA\n\n## Overview\nStreamlineRPA provides robust robotic process automat..."
         ],
         "type": "scatter",
         "x": {
          "bdata": "iyMQwdM2Pb/fWoPBlCNdQe/NaECXFhlCwrkfQndArkEh+VxCGM3ZQSbDucEzozjBMY8ywvAOIcLrGh5CO9DuQWA9gMFA5ivBLEvwPEbKxD8h0EzCGK5twl0tksELs5TBRZRUQeIRv0A6YRVCNDrUQS8ZFcI=",
          "dtype": "f4"
         },
         "y": {
          "bdata": "4X6owlxqlMK+O3TCEBmewocYtML0A8BCjdncQk38vkInSMpC3fvXQvBWJMKmBhfCvQcJwX3yAEBwk048SW3XQIwDRcF5mgpBDWkjwT9XPkD8sBhCA9b3QQN6NELPNgdCmLX8QbYnoUEif7bB5J7Vwbu2JMI=",
          "dtype": "f4"
         }
        }
       ],
       "layout": {
        "height": 600,
        "margin": {
         "b": 10,
         "l": 10,
         "r": 20,
         "t": 40
        },
        "scene": {
         "xaxis": {
          "title": {
           "text": "x"
          }
         },
         "yaxis": {
          "title": {
           "text": "y"
          }
         }
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "2D Chroma Vector Store Visualization"
        },
        "width": 800
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "perplexity = max(5, min(30, len(vectors) // 3))\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity= perplexity)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a69372",
   "metadata": {},
   "source": [
    "<!-- ### Use langchain to bring it all together -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48832433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new chat with OpenAI\n",
    "llm = ChatOpenAI(model_name = MODEL)\n",
    "\n",
    "# set up the converstation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key= 'chat_history', return_messages= True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm = ChatOpenAI(model= MODEL),\n",
    "    retriever = retriever,\n",
    "    chain_type = 'stuff'\n",
    ")\n",
    "\n",
    "# set the rag chain as a function to pass it into a tool\n",
    "def query_company_knowledge(query):\n",
    "    return rag_chain.invoke(query)\n",
    "\n",
    "\n",
    "# set up the tools (web search and specific company queries)\n",
    "\n",
    "tools = [Tool(\n",
    "    name='CompanyKnowledge',\n",
    "    func= query_company_knowledge,\n",
    "    description= \"Use this to answer questions about company data and strategy.\"),\n",
    "Tool(\n",
    "    name= 'WebSearch',\n",
    "    func= DuckDuckGoSearchRun().run,\n",
    "    description= 'use this when you need to perfom a web search or search latest news related to the company')\n",
    "]\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model = MODEL)\n",
    "\n",
    "\n",
    "# This has the correct format with all required variables\n",
    "# prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an AI strategist working at ColumbusAI Solutions.\n",
    "Your mission is to analyze company data, propose innovative AI-based products,\n",
    "and stay updated with the latest AI trends.\n",
    "\n",
    "You have access to two tools:\n",
    "1. 'CompanyKnowledge' — retrieves internal information from the company's data.\n",
    "2. 'WebSearch' — searches the web for recent or external information.\n",
    "\n",
    "If you receive a query about a person, event, or fact not found in internal data,\n",
    "you **must** use the WebSearch tool before answering.\n",
    "If both tools return nothing, respond with:\n",
    "\"I don't have reliable information on that yet.\"\n",
    "\n",
    "Always reason clearly, use tools first, and only synthesize insights afterward.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Create the agent using the built-in function\n",
    "agent = create_openai_tools_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Set up memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    "    output_key='output'\n",
    ")\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "10ab5ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mMy pick would be Santiago Rojas.\n",
      "\n",
      "Why Santiago:\n",
      "- Role fit: Data Analyst with a proven track record in analytics, insights, and cross-team collaboration.\n",
      "- Visualization skills: Proficient in Tableau and Power BI; strong data storytelling and visualization capabilities.\n",
      "- Technical grounding: SQL, Python, R; experience with ETL and data wrangling, which helps in building reliable, end-to-end dashboards.\n",
      "- Certifications and experience: Tableau Desktop Specialist; has worked as a data analyst in prior roles and at our company, which means he knows our data context.\n",
      "\n",
      "Runner-ups:\n",
      "- Helena Suarez (Senior Data Engineer): Excellent for dashboards that require robust data pipelines, governance, and scalable data sources. She’d be ideal as a data-source enabler and for dashboards needing strong data quality and pipeline design.\n",
      "- Samuel Etoo (Senior Accountant): Best for finance-focused dashboards (GAAP reporting, budgeting, consolidation) where domain expertise is critical.\n",
      "\n",
      "If you’d like, I can propose a quick pilot: pick a dashboard use-case (e.g., monthly executive finance and ops dashboard), assign Santiago as lead with Helena handling data sources, set success metrics, and outline deliverables and timeline.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "My pick would be Santiago Rojas.\n",
      "\n",
      "Why Santiago:\n",
      "- Role fit: Data Analyst with a proven track record in analytics, insights, and cross-team collaboration.\n",
      "- Visualization skills: Proficient in Tableau and Power BI; strong data storytelling and visualization capabilities.\n",
      "- Technical grounding: SQL, Python, R; experience with ETL and data wrangling, which helps in building reliable, end-to-end dashboards.\n",
      "- Certifications and experience: Tableau Desktop Specialist; has worked as a data analyst in prior roles and at our company, which means he knows our data context.\n",
      "\n",
      "Runner-ups:\n",
      "- Helena Suarez (Senior Data Engineer): Excellent for dashboards that require robust data pipelines, governance, and scalable data sources. She’d be ideal as a data-source enabler and for dashboards needing strong data quality and pipeline design.\n",
      "- Samuel Etoo (Senior Accountant): Best for finance-focused dashboards (GAAP reporting, budgeting, consolidation) where domain expertise is critical.\n",
      "\n",
      "If you’d like, I can propose a quick pilot: pick a dashboard use-case (e.g., monthly executive finance and ops dashboard), assign Santiago as lead with Helena handling data sources, set success metrics, and outline deliverables and timeline.\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"who do you think could make the best dashboard in the current employees?\"})\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67ffcf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juanj\\AppData\\Local\\Temp\\ipykernel_8536\\2998881634.py:157: UserWarning:\n",
      "\n",
      "You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "\n",
      "C:\\Users\\juanj\\AppData\\Local\\Temp\\ipykernel_8536\\2998881634.py:157: DeprecationWarning:\n",
      "\n",
      "The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanj\\Desktop\\Projects\\llm_engineering\\venv\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning:\n",
      "\n",
      "This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "\n",
      "c:\\Users\\juanj\\Desktop\\Projects\\llm_engineering\\venv\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning:\n",
      "\n",
      "This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "\n",
      "c:\\Users\\juanj\\Desktop\\Projects\\llm_engineering\\venv\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning:\n",
      "\n",
      "This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "\n",
      "c:\\Users\\juanj\\Desktop\\Projects\\llm_engineering\\venv\\Lib\\site-packages\\langchain_community\\utilities\\duckduckgo_search.py:63: RuntimeWarning:\n",
      "\n",
      "This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool, AgentExecutor, create_openai_tools_agent\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import time\n",
    "\n",
    "# Assuming MODEL and vectorstore are already defined\n",
    "# MODEL = \"gpt-4\"\n",
    "# vectorstore = ... (your vector store)\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=MODEL),\n",
    "    retriever=retriever,\n",
    "    chain_type='stuff'\n",
    ")\n",
    "\n",
    "def query_company_knowledge(query: str) -> str:\n",
    "    \"\"\"Query internal company knowledge base.\"\"\"\n",
    "    return rag_chain.run(query)\n",
    "\n",
    "# Set up the tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='CompanyKnowledge',\n",
    "        func=query_company_knowledge,\n",
    "        description=\"Use this to answer questions about company data and strategy.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name='WebSearch',\n",
    "        func=DuckDuckGoSearchRun().run,\n",
    "        description='Use this when you need to perform a web search or search latest news.'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an AI strategist working at ColumbusAI Solutions.\n",
    "Your mission is to analyze company data, propose innovative AI-based products,\n",
    "and stay updated with the latest AI trends.\n",
    "\n",
    "You have access to two tools:\n",
    "1. 'CompanyKnowledge' — retrieves internal information from the company's data.\n",
    "2. 'WebSearch' — searches the web for recent or external information.\n",
    "\n",
    "If you receive a query about a person, event, or fact not found in internal data,\n",
    "you must use the WebSearch tool before answering.\n",
    "If both tools return nothing, respond with: \"I don't have reliable information on that yet.\"\n",
    "\n",
    "Always reason clearly, use tools first, and only synthesize insights afterward.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Global memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    "    output_key='output'\n",
    ")\n",
    "\n",
    "def chat_with_agent(message, history):\n",
    "    \"\"\"\n",
    "    Process user message and yield streaming response with status updates.\n",
    "    \"\"\"\n",
    "    if not message.strip():\n",
    "        yield \"\", \"⚠️ Please enter a message\"\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Show initial status\n",
    "        yield \"\", \"🤔 Thinking...\"\n",
    "        \n",
    "        # Create LLM\n",
    "        llm = ChatOpenAI(model=MODEL, temperature=0)\n",
    "        \n",
    "        # Create agent\n",
    "        agent = create_openai_tools_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "        \n",
    "        # Create agent executor\n",
    "        agent_executor = AgentExecutor(\n",
    "            agent=agent,\n",
    "            tools=tools,\n",
    "            memory=memory,\n",
    "            verbose=False,  # Set True to see logs\n",
    "            handle_parsing_errors=True,\n",
    "            max_iterations=5,\n",
    "            return_intermediate_steps=True\n",
    "        )\n",
    "        \n",
    "        # Status update: Searching\n",
    "        yield \"\", \"🔍 Searching for information...\"\n",
    "        \n",
    "        # Invoke agent\n",
    "        response = agent_executor.invoke({\"input\": message})\n",
    "        \n",
    "        # Check which tools were used\n",
    "        intermediate_steps = response.get('intermediate_steps', [])\n",
    "        tools_used = []\n",
    "        if intermediate_steps:\n",
    "            for step in intermediate_steps:\n",
    "                tool_name = step[0].tool\n",
    "                tools_used.append(tool_name)\n",
    "        \n",
    "        # Create status message\n",
    "        if tools_used:\n",
    "            tools_str = \", \".join(set(tools_used))\n",
    "            status = f\"✅ Used: {tools_str}\"\n",
    "        else:\n",
    "            status = \"✅ Response generated\"\n",
    "        \n",
    "        # Get final response\n",
    "        final_response = response['output']\n",
    "        \n",
    "        # Stream the response character by character\n",
    "        streamed_text = \"\"\n",
    "        for i, char in enumerate(final_response):\n",
    "            streamed_text += char\n",
    "            # Update every few characters for smoother streaming\n",
    "            if i % 3 == 0 or i == len(final_response) - 1:\n",
    "                yield streamed_text, status\n",
    "                time.sleep(0.01)  # Small delay for visual effect\n",
    "        \n",
    "        # Final yield with complete response\n",
    "        yield final_response, status\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error: {str(e)}\"\n",
    "        yield \"\", error_msg\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset conversation memory.\"\"\"\n",
    "    global memory\n",
    "    memory.clear()\n",
    "    return [], \"\", \"💬 Conversation cleared. Ready for new questions!\"\n",
    "\n",
    "# Create Gradio interface with Blocks for more control\n",
    "with gr.Blocks(title=\"ColumbusAI Solutions\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    # Header\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🤖 ColumbusAI Solutions - AI Strategist Assistant\n",
    "    \n",
    "    Your intelligent assistant for company insights, AI trends, and strategic recommendations.\n",
    "    \"\"\")\n",
    "    \n",
    "    # Main chat interface\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                label=\"Chat History\",\n",
    "                avatar_images=(None, \"🤖\"),\n",
    "                bubble_full_width=False,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            # Status indicator\n",
    "            status = gr.Textbox(\n",
    "                label=\"Status\",\n",
    "                value=\"💬 Ready to chat!\",\n",
    "                interactive=False,\n",
    "                max_lines=1\n",
    "            )\n",
    "            \n",
    "            # Input area\n",
    "            with gr.Row():\n",
    "                msg = gr.Textbox(\n",
    "                    label=\"Your Message\",\n",
    "                    placeholder=\"Ask me anything...\",\n",
    "                    lines=2,\n",
    "                    scale=4\n",
    "                )\n",
    "                submit = gr.Button(\"Send 📤\", variant=\"primary\", scale=1)\n",
    "            \n",
    "            # Action buttons\n",
    "            with gr.Row():\n",
    "                clear = gr.Button(\"🗑️ Clear Chat\")\n",
    "                retry = gr.Button(\"🔄 Retry Last\")\n",
    "        \n",
    "        # Sidebar with examples and info\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"\"\"\n",
    "            ### 💡 Example Questions\n",
    "            \n",
    "            **Company Info:**\n",
    "            - What is our AI strategy?\n",
    "            - Tell me about our products\n",
    "            \n",
    "            **People:**\n",
    "            - Who is Juan Rojas?\n",
    "            \n",
    "            **Trends:**\n",
    "            - Latest AI developments\n",
    "            - AI market trends 2024\n",
    "            \n",
    "            **Innovation:**\n",
    "            - Suggest new AI products\n",
    "            - Competitive analysis\n",
    "            \n",
    "            ---\n",
    "            \n",
    "            ### 🔧 Features\n",
    "            - ✅ Real-time streaming\n",
    "            - 🔍 Internal knowledge search\n",
    "            - 🌐 Web search capability\n",
    "            - 💾 Conversation memory\n",
    "            - 📊 Status indicators\n",
    "            \"\"\")\n",
    "    \n",
    "    # Event handlers\n",
    "    def submit_message(message, history):\n",
    "        \"\"\"Handle message submission.\"\"\"\n",
    "        # Add user message to history\n",
    "        history = history + [[message, None]]\n",
    "        return \"\", history\n",
    "    \n",
    "    def bot_response(history):\n",
    "        \"\"\"Generate bot response with streaming.\"\"\"\n",
    "        if not history or history[-1][1] is not None:\n",
    "            return history, \"💬 Ready\"\n",
    "        \n",
    "        user_message = history[-1][0]\n",
    "        \n",
    "        # Stream the response\n",
    "        for partial_response, status_msg in chat_with_agent(user_message, history[:-1]):\n",
    "            history[-1][1] = partial_response\n",
    "            yield history, status_msg\n",
    "    \n",
    "    # Connect events\n",
    "    msg.submit(\n",
    "        submit_message,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot]\n",
    "    ).then(\n",
    "        bot_response,\n",
    "        inputs=[chatbot],\n",
    "        outputs=[chatbot, status]\n",
    "    )\n",
    "    \n",
    "    submit.click(\n",
    "        submit_message,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot]\n",
    "    ).then(\n",
    "        bot_response,\n",
    "        inputs=[chatbot],\n",
    "        outputs=[chatbot, status]\n",
    "    )\n",
    "    \n",
    "    clear.click(\n",
    "        reset_conversation,\n",
    "        outputs=[chatbot, msg, status]\n",
    "    )\n",
    "    \n",
    "    retry.click(\n",
    "        lambda hist: (hist[:-1] if hist else [], hist[-1][0] if hist else \"\"),\n",
    "        inputs=[chatbot],\n",
    "        outputs=[chatbot, msg]\n",
    "    )\n",
    "    \n",
    "    # Footer\n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    <center>\n",
    "    <small>Powered by LangChain + OpenAI | ColumbusAI Solutions © 2024</small>\n",
    "    </center>\n",
    "    \"\"\")\n",
    "\n",
    "# Launch\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        # server_name=\"0.0.0.0\",\n",
    "        # server_port=7860,\n",
    "        share=False,  # Set True to create public link\n",
    "        debug=True,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba13882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': '- \"hello\"\\n- \"what was my previous prompt?\"\\n- \"who is Juan Rojas?\"\\n- \"who is Juan Rojas in my company?\"'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\"http://127.0.0.1:8000/chat\", json={\"query\": \"list my 4 previous prompts please?\"})\n",
    "print(response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
